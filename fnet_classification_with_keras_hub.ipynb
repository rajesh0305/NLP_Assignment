{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajesh0305/NLP_Assignment/blob/main/fnet_classification_with_keras_hub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j8lnXV2Z_Jk"
      },
      "source": [
        "# Text Classification using FNet\n",
        "\n",
        "**Author:** [Rajesh Kumar](https://github.com/rajesh0305)<br>\n",
        "**Date created:** 2025/02/02<br>\n",
        "**Last modified:** 2022/02/12<br>\n",
        "**Description:** Text Classification on the IMDb Dataset using `keras_hub.layers.FNetEncoder` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEPp84lDZ_Jt"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this example, we will demonstrate the ability of FNet to achieve comparable\n",
        "results with a vanilla Transformer model on the text classification task.\n",
        "We will be using the IMDb dataset, which is a\n",
        "collection of movie reviews labelled either positive or negative (sentiment\n",
        "analysis).\n",
        "\n",
        "To build the tokenizer, model, etc., we will use components from\n",
        "[KerasHub](https://github.com/keras-team/keras-hub). KerasHub makes life easier\n",
        "for people who want to build NLP pipelines! :)\n",
        "\n",
        "### Model\n",
        "\n",
        "Transformer-based language models (LMs) such as BERT, RoBERTa, XLNet, etc. have\n",
        "demonstrated the effectiveness of the self-attention mechanism for computing\n",
        "rich embeddings for input text. However, the self-attention mechanism is an\n",
        "expensive operation, with a time complexity of `O(n^2)`, where `n` is the number\n",
        "of tokens in the input. Hence, there has been an effort to reduce the time\n",
        "complexity of the self-attention mechanism and improve performance without\n",
        "sacrificing the quality of results.\n",
        "\n",
        "In 2020, a paper titled\n",
        "[FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824)\n",
        "replaced the self-attention layer in BERT with a simple Fourier Transform layer\n",
        "for \"token mixing\". This resulted in comparable accuracy and a speed-up during\n",
        "training. In particular, a couple of points from the paper stand out:\n",
        "\n",
        "* The authors claim that FNet is 80% faster than BERT on GPUs and 70% faster on\n",
        "TPUs. The reason for this speed-up is two-fold: a) the Fourier Transform layer\n",
        "is unparametrized, it does not have any parameters, and b) the authors use Fast\n",
        "Fourier Transform (FFT); this reduces the time complexity from `O(n^2)`\n",
        "(in the case of self-attention) to `O(n log n)`.\n",
        "* FNet manages to achieve 92-97% of the accuracy of BERT on the GLUE benchmark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eQ6VKa_Z_Ju"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Before we start with the implementation, let's import all the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45iBDIV1Z_Jv"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade keras-hub\n",
        "!pip install -q --upgrade keras  # Upgrade to Keras 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPuM-z5oZ_Jx"
      },
      "outputs": [],
      "source": [
        "import keras_hub\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "keras.utils.set_random_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHjvtrdPZ_Jy"
      },
      "source": [
        "Let's also define our hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag_BS-vKZ_Jz"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "MAX_SEQUENCE_LENGTH = 512\n",
        "VOCAB_SIZE = 15000\n",
        "\n",
        "EMBED_DIM = 128\n",
        "INTERMEDIATE_DIM = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G--9GeFZ_J0"
      },
      "source": [
        "## Loading the dataset\n",
        "\n",
        "First, let's download the IMDB dataset and extract it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpgnkDb0Z_J0",
        "outputId": "cd61b39e-c729-439b-8fff-b9d0c0bae457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-12 06:46:40--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz.3’\n",
            "\n",
            "aclImdb_v1.tar.gz.3 100%[===================>]  80.23M  17.1MB/s    in 13s     \n",
            "\n",
            "2025-02-12 06:46:53 (6.34 MB/s) - ‘aclImdb_v1.tar.gz.3’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS7qxnchZ_J1"
      },
      "source": [
        "Samples are present in the form of text files. Let's inspect the structure of\n",
        "the directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIl-hSHCZ_J1",
        "outputId": "0cf0247d-6a7e-42de-bf89-efbb405bd99e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['test', 'train', 'README', 'imdbEr.txt', 'imdb.vocab']\n",
            "['pos', 'neg', 'unsupBow.feat', 'urls_neg.txt', 'unsup', 'urls_pos.txt', 'urls_unsup.txt', 'labeledBow.feat']\n",
            "['pos', 'neg', 'urls_neg.txt', 'urls_pos.txt', 'labeledBow.feat']\n"
          ]
        }
      ],
      "source": [
        "print(os.listdir(\"./aclImdb\"))\n",
        "print(os.listdir(\"./aclImdb/train\"))\n",
        "print(os.listdir(\"./aclImdb/test\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu8-SMfXZ_J2"
      },
      "source": [
        "The directory contains two sub-directories: `train` and `test`. Each subdirectory\n",
        "in turn contains two folders: `pos` and `neg` for positive and negative reviews,\n",
        "respectively. Before we load the dataset, let's delete the `./aclImdb/train/unsup`\n",
        "folder since it has unlabelled samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slJ9n4FCZ_J3"
      },
      "outputs": [],
      "source": [
        "!rm -rf aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MruxuAUTZ_J3"
      },
      "source": [
        "We'll use the `keras.utils.text_dataset_from_directory` utility to generate\n",
        "our labelled `tf.data.Dataset` dataset from text files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYEo3RrcZ_J4",
        "outputId": "cdce46a4-8e05-40be-c96b-7338a7dd5b9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=42,\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=42,\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoAbjbXJZ_J5"
      },
      "source": [
        "We will now convert the text to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcwylX6FZ_J6"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.map(lambda x, y: (tf.strings.lower(x), y))\n",
        "val_ds = val_ds.map(lambda x, y: (tf.strings.lower(x), y))\n",
        "test_ds = test_ds.map(lambda x, y: (tf.strings.lower(x), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tykFDKVCZ_J7"
      },
      "source": [
        "Let's print a few samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBLJdJliZ_J7",
        "outputId": "b0a3794c-a0d2-4eb0-edfd-7b3943415cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"mild spoilers<br /><br />in the near future, arnold stars as ben richards, a wrongly convicted man coerced into playing 'the running man', a deadly tv game show where people have to keep moving to try and escape brutal deaths at the hands of the 'stalkers'. of course, people are expected to die eventually and its up to arnold to prove the system wrong.<br /><br />i haven't read the stephen king book, but this is a great film regardless, one of arnold's best. he does what he does best in the action man role, delivering death with unforgettable one-liners. classics are probably the 'he was a real pain in the neck' after strangling a guy with barb wire, and 'he had to split!', referring to whereabouts he just chain sawed someone vertically. dawson is perfectly irritating as the tv presenter, and all the 'stalkers' are suitably camp. the action is violent, but its an action film. that's the point. the film is fast paced, and at 90 minutes it doesn't overstay its welcome. <br /><br />with starsky and hutch's paul michael glaser at the helm, and made in the wake of the success of the terminator, previously this film was probably seen as just another mindless action vehicle for arnold, and very far fetched. but today, anyone who watches a lot of tv could see how the film is getting closer to reality. i wouldn't be surprised if i turn on the tv in the 'near future' and see a show not to far from this.<br /><br />on that depressing note, i must however recommend 'the running man' to anyone who likes the 80s, arnold, ridiculous acts or violence or just a good action film. 9. 5 / 10\"\n",
            "1\n",
            "b\"to get in touch with the beauty of this film pay close attention to the sound track, not only the music, but the way all sounds help to weave the imagery. how beautifully the opening scene leading to the expulsion of gino establishes the theme of moral ambiguity! note the way music introduces the characters as we are led inside giovanna's marriage. don't expect to find much here of the political life of italy in 1943. that's not what this is about. on the other hand, if you are susceptible to the music of images and sounds, you will be led into a word that reaches beyond neo-realism. by the end of the film we there are moments antonioni-like landscape that has more to do with the inner life of the characters than with real places. this is one of my favorite visconti films.\"\n",
            "1\n",
            "b'it is rare that one comes across a movie as flawless as this. it\\'s truly one of the best acted, most tightly structured films i\\'ve ever seen. every line of dialogue can be interpreted in several ways, relating to each of the three main characters differently. the film weaves an intrinsic web of motivations and double crosses that snare you and refuse to let go. add to this that the slow-burning romance between kevin and faye is as moving as anything that\\'s ever been committed to celluloid and you have the ingredients for a perfect film. it exposes the romance of movies such as \"titanic\" as the trite cliches they are. if you\\'re looking for a movie to watch while you fold laundry, this isn\\'t it. you have to commit yourself to this film. you can\\'t have a conversation while running in and out of the room. this movie demands your attention. treat it with the respect you deserve and you\\'ll get a lot out of it. unless you think \"titanic\" is the greatest film ever.'\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "    for i in range(3):\n",
        "        print(text_batch.numpy()[i])\n",
        "        print(label_batch.numpy()[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QedndJ-JZ_J8"
      },
      "source": [
        "### Tokenizing the data\n",
        "\n",
        "We'll be using the `keras_hub.tokenizers.WordPieceTokenizer` layer to tokenize\n",
        "the text. `keras_hub.tokenizers.WordPieceTokenizer` takes a WordPiece vocabulary\n",
        "and has functions for tokenizing the text, and detokenizing sequences of tokens.\n",
        "\n",
        "Before we define the tokenizer, we first need to train it on the dataset\n",
        "we have. The WordPiece tokenization algorithm is a subword tokenization algorithm;\n",
        "training it on a corpus gives us a vocabulary of subwords. A subword tokenizer\n",
        "is a compromise between word tokenizers (word tokenizers need very large\n",
        "vocabularies for good coverage of input words), and character tokenizers\n",
        "(characters don't really encode meaning like words do). Luckily, KerasHub\n",
        "makes it very simple to train WordPiece on a corpus with the\n",
        "`keras_hub.tokenizers.compute_word_piece_vocabulary` utility.\n",
        "\n",
        "Note: The official implementation of FNet uses the SentencePiece Tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGsOP8p1Z_J9"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_word_piece(ds, vocab_size, reserved_tokens):\n",
        "    word_piece_ds = ds.unbatch().map(lambda x, y: x)\n",
        "    vocab = keras_hub.tokenizers.compute_word_piece_vocabulary(\n",
        "        word_piece_ds.batch(1000).prefetch(2),\n",
        "        vocabulary_size=vocab_size,\n",
        "        reserved_tokens=reserved_tokens,\n",
        "    )\n",
        "    return vocab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkuHJ4UJZ_J9"
      },
      "source": [
        "Every vocabulary has a few special, reserved tokens. We have two such tokens:\n",
        "\n",
        "- `\"[PAD]\"` - Padding token. Padding tokens are appended to the input sequence length\n",
        "when the input sequence length is shorter than the maximum sequence length.\n",
        "- `\"[UNK]\"` - Unknown token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zm0xm__Z_J9"
      },
      "outputs": [],
      "source": [
        "reserved_tokens = [\"[PAD]\", \"[UNK]\"]\n",
        "train_sentences = [element[0] for element in train_ds]\n",
        "vocab = train_word_piece(train_ds, VOCAB_SIZE, reserved_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-f6iqbzZ_J9"
      },
      "source": [
        "Let's see some tokens!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXacVl7CZ_J9",
        "outputId": "704638c5-d81f-4eac-f9d4-532f97375519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:  ['à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é']\n"
          ]
        }
      ],
      "source": [
        "print(\"Tokens: \", vocab[100:110])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjnFQaI2Z_J-"
      },
      "source": [
        "Now, let's define the tokenizer. We will configure the tokenizer with the\n",
        "the vocabularies trained above. We will define a maximum sequence length so that\n",
        "all sequences are padded to the same length, if the length of the sequence is\n",
        "less than the specified sequence length. Otherwise, the sequence is truncated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "or7jjUL4Z_J-"
      },
      "outputs": [],
      "source": [
        "tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab,\n",
        "    lowercase=False,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjRS75FoZ_J-"
      },
      "source": [
        "Let's try and tokenize a sample from our dataset! To verify whether the text has\n",
        "been tokenized correctly, we can also detokenize the list of tokens back to the\n",
        "original text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxPFFyX5Z_J_",
        "outputId": "7e320a63-98ed-460e-bf81-1d7a080f064c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence:  tf.Tensor(b'this picture seemed way to slanted, it\\'s almost as bad as the drum beating of the right wing kooks who say everything is rosy in iraq. it paints a picture so unredeemable that i can\\'t help but wonder about it\\'s legitimacy and bias. also it seemed to meander from being about the murderous carnage of our troops to the lack of health care in the states for ptsd. to me the subject matter seemed confused, it only cared about portraying the military in a bad light, as a) an organzation that uses mind control to turn ordinary peace loving civilians into baby killers and b) an organization that once having used and spent the bodies of it\\'s soldiers then discards them to the despotic bureacracy of the v.a. this is a legitimate argument, but felt off topic for me, almost like a movie in and of itself. i felt that \"the war tapes\" and \"blood of my brother\" were much more fair and let the viewer draw some conclusions of their own rather than be beaten over the head with the film makers viewpoint. f-', shape=(), dtype=string)\n",
            "Tokens:  tf.Tensor(\n",
            "[  145   576   608   228   140    58 13343    13   143     8    58   360\n",
            "   148   209   148   137  9759  3681   139   137   344  3276    50 12092\n",
            "   164   169   269   424   141    57  2093   292   144  5115    15   143\n",
            "  7890    40   576   170  2970  2459  2412 10452   146    48   184     8\n",
            "    59   478   152   733   177   143     8    58  4060  8069 13355   138\n",
            "  8557    15   214   143   608   140   526  2121   171   247   177   137\n",
            "  4726  7336   139   395  4985   140   137   711   139  3959   597   144\n",
            "   137  1844   149    55  1175   288    15   140   203   137  1009   686\n",
            "   608  1701    13   143   197  3979   177  2514   137  1442   144    40\n",
            "   209   776    13   148    40    10   168 14198 13928   146  1260   470\n",
            "  1300   140   604  2118  2836  1873  9991   217  1006  2318   138    41\n",
            "    10   168  8469   146   422   400   480   138  1213   137  2541   139\n",
            "   143     8    58  1487   227  4319 10720   229   140   137  6310  8532\n",
            "   862    41  2215  6547 10768   139   137    61    15    40    15   145\n",
            "   141    40  7738  4120    13   152   569   260  3297   149   203    13\n",
            "   360   172    40   150   144   138   139   561    15    48   569   146\n",
            "     3   137   466  6192     3   138     3   665   139   193   707     3\n",
            "   204   207   185  1447   138   417   137   643  2731   182  8421   139\n",
            "   199   342   385   206   161  3920   253   137   566   151   137   153\n",
            "  1340  8845    15    45    14     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0], shape=(512,), dtype=int32)\n",
            "Recovered text after detokenizing:  this picture seemed way to slanted , it ' s almost as bad as the drum beating of the right wing kooks who say everything is rosy in iraq . it paints a picture so unredeemable that i can ' t help but wonder about it ' s legitimacy and bias . also it seemed to meander from being about the murderous carnage of our troops to the lack of health care in the states for ptsd . to me the subject matter seemed confused , it only cared about portraying the military in a bad light , as a ) an organzation that uses mind control to turn ordinary peace loving civilians into baby killers and b ) an organization that once having used and spent the bodies of it ' s soldiers then discards them to the despotic bureacracy of the v . a . this is a legitimate argument , but felt off topic for me , almost like a movie in and of itself . i felt that \" the war tapes \" and \" blood of my brother \" were much more fair and let the viewer draw some conclusions of their own rather than be beaten over the head with the film makers viewpoint . f - [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ],
      "source": [
        "input_sentence_ex = train_ds.take(1).get_single_element()[0][0]\n",
        "input_tokens_ex = tokenizer(input_sentence_ex)\n",
        "\n",
        "print(\"Sentence: \", input_sentence_ex)\n",
        "print(\"Tokens: \", input_tokens_ex)\n",
        "print(\"Recovered text after detokenizing: \", tokenizer.detokenize(input_tokens_ex))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ55LVp4Z_J_"
      },
      "source": [
        "## Formatting the dataset\n",
        "\n",
        "Next, we'll format our datasets in the form that will be fed to the models. We\n",
        "need to tokenize the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MbjY-HnZ_J_"
      },
      "outputs": [],
      "source": [
        "\n",
        "def format_dataset(sentence, label):\n",
        "    sentence = tokenizer(sentence)\n",
        "    return ({\"input_ids\": sentence}, label)\n",
        "\n",
        "\n",
        "def make_dataset(dataset):\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.shuffle(512).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_ds)\n",
        "val_ds = make_dataset(val_ds)\n",
        "test_ds = make_dataset(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVoYDbktZ_KA"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "Now, let's move on to the exciting part - defining our model!\n",
        "We first need an embedding layer, i.e., a layer that maps every token in the input\n",
        "sequence to a vector. This embedding layer can be initialised randomly. We also\n",
        "need a positional embedding layer which encodes the word order in the sequence.\n",
        "The convention is to add, i.e., sum, these two embeddings. KerasHub has a\n",
        "`keras_hub.layers.TokenAndPositionEmbedding ` layer which does all of the above\n",
        "steps for us.\n",
        "\n",
        "Our FNet classification model consists of three `keras_hub.layers.FNetEncoder`\n",
        "layers with a `keras.layers.Dense` layer on top.\n",
        "\n",
        "Note: For FNet, masking the padding tokens has a minimal effect on results. In the\n",
        "official implementation, the padding tokens are not masked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBWeDXhqZ_KA",
        "outputId": "b96ac351-0ff6-45fd-86ef-21e89ad4d973"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'f_net_encoder_6' (of type FNetEncoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "input_ids = keras.Input(shape=(None,), dtype=\"int64\", name=\"input_ids\")\n",
        "\n",
        "x = keras_hub.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    mask_zero=True,\n",
        ")(input_ids)\n",
        "\n",
        "x = keras_hub.layers.FNetEncoder(intermediate_dim=INTERMEDIATE_DIM)(inputs=x)\n",
        "x = keras_hub.layers.FNetEncoder(intermediate_dim=INTERMEDIATE_DIM)(inputs=x)\n",
        "x = keras_hub.layers.FNetEncoder(intermediate_dim=INTERMEDIATE_DIM)(inputs=x)\n",
        "\n",
        "\n",
        "x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "x = keras.layers.Dropout(0.1)(x)\n",
        "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "fnet_classifier = keras.Model(input_ids, outputs, name=\"fnet_classifier\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE-oDhGVZ_KB"
      },
      "source": [
        "## Training our model\n",
        "\n",
        "We'll use accuracy to monitor training progress on the validation data. Let's\n",
        "train our model for 54\n",
        "epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ny_5YLE_Z_KB",
        "outputId": "c7597af4-e4c3-4e60-d728-05fcd4bbb3d2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"fnet_classifier\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"fnet_classifier\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_ids (\u001b[38;5;33mInputLayer\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding_6       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │       \u001b[38;5;34m1,985,536\u001b[0m │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ f_net_encoder_6 (\u001b[38;5;33mFNetEncoder\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │         \u001b[38;5;34m132,224\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ f_net_encoder_7 (\u001b[38;5;33mFNetEncoder\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │         \u001b[38;5;34m132,224\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ f_net_encoder_8 (\u001b[38;5;33mFNetEncoder\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │         \u001b[38;5;34m132,224\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling1d_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_18 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding_6       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,985,536</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ f_net_encoder_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">FNetEncoder</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">132,224</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ f_net_encoder_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">FNetEncoder</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">132,224</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ f_net_encoder_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">FNetEncoder</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">132,224</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling1d_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,382,337\u001b[0m (9.09 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,382,337</span> (9.09 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,382,337\u001b[0m (9.09 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,382,337</span> (9.09 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 27ms/step - accuracy: 0.6257 - loss: 0.6252 - val_accuracy: 0.8490 - val_loss: 0.3504\n",
            "Epoch 2/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.8678 - loss: 0.3128 - val_accuracy: 0.8568 - val_loss: 0.3556\n",
            "Epoch 3/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 24ms/step - accuracy: 0.9293 - loss: 0.1884 - val_accuracy: 0.8112 - val_loss: 0.5494\n",
            "Epoch 4/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9512 - loss: 0.1307 - val_accuracy: 0.8516 - val_loss: 0.4259\n",
            "Epoch 5/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9627 - loss: 0.0959 - val_accuracy: 0.8406 - val_loss: 0.5701\n",
            "Epoch 6/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9740 - loss: 0.0699 - val_accuracy: 0.8460 - val_loss: 0.5876\n",
            "Epoch 7/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - accuracy: 0.9764 - loss: 0.0644 - val_accuracy: 0.8198 - val_loss: 0.7666\n",
            "Epoch 8/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9744 - loss: 0.0707 - val_accuracy: 0.8106 - val_loss: 0.8479\n",
            "Epoch 9/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - accuracy: 0.9739 - loss: 0.0702 - val_accuracy: 0.8294 - val_loss: 0.7771\n",
            "Epoch 10/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - accuracy: 0.9713 - loss: 0.0757 - val_accuracy: 0.8170 - val_loss: 0.8869\n",
            "Epoch 11/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9844 - loss: 0.0455 - val_accuracy: 0.8292 - val_loss: 0.8128\n",
            "Epoch 12/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - accuracy: 0.9843 - loss: 0.0413 - val_accuracy: 0.8260 - val_loss: 0.8536\n",
            "Epoch 13/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 23ms/step - accuracy: 0.9922 - loss: 0.0206 - val_accuracy: 0.8374 - val_loss: 0.7486\n",
            "Epoch 14/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 23ms/step - accuracy: 0.9918 - loss: 0.0230 - val_accuracy: 0.8318 - val_loss: 0.9165\n",
            "Epoch 15/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9934 - loss: 0.0183 - val_accuracy: 0.8460 - val_loss: 0.8935\n",
            "Epoch 16/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 24ms/step - accuracy: 0.9939 - loss: 0.0162 - val_accuracy: 0.8420 - val_loss: 0.8147\n",
            "Epoch 17/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - accuracy: 0.9895 - loss: 0.0305 - val_accuracy: 0.8464 - val_loss: 0.8256\n",
            "Epoch 18/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.9943 - loss: 0.0172 - val_accuracy: 0.8386 - val_loss: 0.8715\n",
            "Epoch 19/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 23ms/step - accuracy: 0.9916 - loss: 0.0236 - val_accuracy: 0.8424 - val_loss: 0.8166\n",
            "Epoch 20/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 23ms/step - accuracy: 0.9955 - loss: 0.0128 - val_accuracy: 0.8396 - val_loss: 0.9182\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e0ec2808890>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "fnet_classifier.summary()\n",
        "fnet_classifier.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "fnet_classifier.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62ts1_zQZ_KC"
      },
      "source": [
        "We obtain a train accuracy of around 92% and a validation accuracy of around\n",
        "85%. Moreover, for 3 epochs, it takes around 86 seconds to train the model\n",
        "(on Colab with a 16 GB Tesla T4 GPU).\n",
        "\n",
        "Let's calculate the test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zhAV61IZ_KC",
        "outputId": "48dc8085-d349-44bb-fc61-1d2f877054f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.8220 - loss: 0.9940\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9961047172546387, 0.8216000199317932]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "fnet_classifier.evaluate(test_ds, batch_size=BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs93yROsZ_KC"
      },
      "source": [
        "## Comparison with Transformer model\n",
        "\n",
        "Let's compare our FNet Classifier model with a Transformer Classifier model. We\n",
        "keep all the parameters/hyperparameters the same. For example, we use three\n",
        "`TransformerEncoder` layers.\n",
        "\n",
        "We set the number of heads to 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gjP4zQLTZ_KD",
        "outputId": "ede29d3f-25f0-49a0-e3f6-6a70ba6fef5d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"transformer_classifier\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer_classifier\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_ids (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_and_position_embed… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │      \u001b[38;5;34m1,985,536\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbeddi…\u001b[0m │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder_12    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m197,242\u001b[0m │ token_and_position_em… │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder_13    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m197,242\u001b[0m │ transformer_encoder_1… │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder_14    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m197,242\u001b[0m │ transformer_encoder_1… │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal_7 (\u001b[38;5;33mNotEqual\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling1d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ transformer_encoder_1… │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)  │                        │                │ not_equal_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ global_average_poolin… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m129\u001b[0m │ dropout_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_and_position_embed… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,985,536</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbeddi…</span> │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder_12    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">197,242</span> │ token_and_position_em… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder_13    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">197,242</span> │ transformer_encoder_1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder_14    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">197,242</span> │ transformer_encoder_1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling1d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_encoder_1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)  │                        │                │ not_equal_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_poolin… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,577,391\u001b[0m (9.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,577,391</span> (9.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,577,391\u001b[0m (9.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,577,391</span> (9.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 92ms/step - accuracy: 0.6900 - loss: 0.5762 - val_accuracy: 0.8844 - val_loss: 0.2834\n",
            "Epoch 2/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 82ms/step - accuracy: 0.9163 - loss: 0.2234 - val_accuracy: 0.8886 - val_loss: 0.3675\n",
            "Epoch 3/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 82ms/step - accuracy: 0.9534 - loss: 0.1393 - val_accuracy: 0.8710 - val_loss: 0.4265\n",
            "Epoch 4/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 83ms/step - accuracy: 0.9681 - loss: 0.0972 - val_accuracy: 0.8754 - val_loss: 0.4443\n",
            "Epoch 5/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 82ms/step - accuracy: 0.9779 - loss: 0.0693 - val_accuracy: 0.8422 - val_loss: 0.6166\n",
            "Epoch 6/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 82ms/step - accuracy: 0.9557 - loss: 0.1164 - val_accuracy: 0.8640 - val_loss: 0.4727\n",
            "Epoch 7/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 81ms/step - accuracy: 0.9186 - loss: 0.2077 - val_accuracy: 0.5448 - val_loss: 0.6718\n",
            "Epoch 8/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 81ms/step - accuracy: 0.6501 - loss: 0.5854 - val_accuracy: 0.7648 - val_loss: 0.5027\n",
            "Epoch 9/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 81ms/step - accuracy: 0.8166 - loss: 0.4008 - val_accuracy: 0.4924 - val_loss: 0.7008\n",
            "Epoch 10/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 81ms/step - accuracy: 0.4941 - loss: 0.7011 - val_accuracy: 0.4924 - val_loss: 0.7041\n",
            "Epoch 11/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 81ms/step - accuracy: 0.4959 - loss: 0.6986 - val_accuracy: 0.4924 - val_loss: 0.7047\n",
            "Epoch 12/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 80ms/step - accuracy: 0.4995 - loss: 0.6972 - val_accuracy: 0.4924 - val_loss: 0.7036\n",
            "Epoch 13/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 80ms/step - accuracy: 0.4935 - loss: 0.6971 - val_accuracy: 0.4924 - val_loss: 0.7024\n",
            "Epoch 14/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 80ms/step - accuracy: 0.4960 - loss: 0.6960 - val_accuracy: 0.4924 - val_loss: 0.7022\n",
            "Epoch 15/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 80ms/step - accuracy: 0.4864 - loss: 0.6957 - val_accuracy: 0.4924 - val_loss: 0.7010\n",
            "Epoch 16/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 82ms/step - accuracy: 0.4886 - loss: 0.6952 - val_accuracy: 0.4924 - val_loss: 0.6996\n",
            "Epoch 17/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 80ms/step - accuracy: 0.4968 - loss: 0.6949 - val_accuracy: 0.4924 - val_loss: 0.6983\n",
            "Epoch 18/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 82ms/step - accuracy: 0.5004 - loss: 0.6945 - val_accuracy: 0.4924 - val_loss: 0.6966\n",
            "Epoch 19/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 80ms/step - accuracy: 0.4955 - loss: 0.6943 - val_accuracy: 0.4924 - val_loss: 0.6950\n",
            "Epoch 20/20\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 80ms/step - accuracy: 0.4997 - loss: 0.6938 - val_accuracy: 0.4924 - val_loss: 0.6941\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e0ebe4c5b50>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "NUM_HEADS = 6\n",
        "input_ids = keras.Input(shape=(None,), dtype=\"int64\", name=\"input_ids\")\n",
        "\n",
        "\n",
        "x = keras_hub.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    mask_zero=True,\n",
        ")(input_ids)\n",
        "\n",
        "x = keras_hub.layers.TransformerEncoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(inputs=x)\n",
        "x = keras_hub.layers.TransformerEncoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(inputs=x)\n",
        "x = keras_hub.layers.TransformerEncoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(inputs=x)\n",
        "\n",
        "\n",
        "x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "x = keras.layers.Dropout(0.1)(x)\n",
        "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "transformer_classifier = keras.Model(input_ids, outputs, name=\"transformer_classifier\")\n",
        "\n",
        "\n",
        "transformer_classifier.summary()\n",
        "transformer_classifier.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "transformer_classifier.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0bLCD6oZ_KL"
      },
      "source": [
        "We obtain a train accuracy of around 94% and a validation accuracy of around\n",
        "86.5%. It takes around 146 seconds to train the model (on Colab with a 16 GB Tesla\n",
        "T4 GPU).\n",
        "\n",
        "Let's calculate the test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYVahhBeZ_KL",
        "outputId": "7d027bbf-ae4f-46fa-ce2a-b7db7c882e43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 28ms/step - accuracy: 0.4960 - loss: 0.6939\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.693645715713501, 0.5]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "transformer_classifier.evaluate(test_ds, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOWBEyvKZ_KL"
      },
      "source": [
        "Let's make a table and compare the two models. We can see that FNet\n",
        "significantly speeds up our run time (1.7x), with only a small sacrifice in\n",
        "overall accuracy (drop of 0.75%).\n",
        "\n",
        "|                         | **FNet Classifier** | **Transformer Classifier** |\n",
        "|:-----------------------:|:-------------------:|:--------------------------:|\n",
        "|    **Training Time**    |      86 seconds     |         146 seconds        |\n",
        "|    **Train Accuracy**   |        92.34%       |           93.85%           |\n",
        "| **Validation Accuracy** |        85.21%       |           86.42%           |\n",
        "|    **Test Accuracy**    |        83.94%       |           84.69%           |\n",
        "|       **#Params**       |      2,321,921      |          2,520,065         |"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "fnet_classification_with_keras_hub",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}